{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from proto import *\n",
    "from math import sqrt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Reshape, LeakyReLU\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_H, GRID_W = 6, 6\n",
    "\n",
    "IMAGE_SIZE = (4000, 6000)\n",
    "BATCH_SIZE = 2\n",
    "WARM_UP_BATCHES = 0\n",
    "\n",
    "NO_OBJECT_SCALE = 1.\n",
    "OBJECT_SCALE = 5.\n",
    "COORD_SCALE = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r'..\\..\\ImageGenerator\\Images'\n",
    "data_path = r'..\\..\\ImageGenerator\\Image Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(image_path, json_path):\n",
    "    training_data = []\n",
    "    for img in tqdm(os.listdir(image_path)):\n",
    "        try:\n",
    "            img_array = cv2.imread(os.path.join(image_path, img))\n",
    "            img_array = cv2.resize(img_array, (6000, 6000), interpolation=cv2.INTER_AREA)\n",
    "        except:\n",
    "            continue\n",
    "        json_file = img[0:-4] + '.json'\n",
    "        json_dict = json.load(open(os.path.join(json_path, json_file)))\n",
    "        grid_data = np.zeros((GRID_H, GRID_W, 5))\n",
    "        for target in json_dict:\n",
    "            x = json_dict[target]['x'] + 0.5 * json_dict[target]['width']  # move x, y to middle of target\n",
    "            y = json_dict[target]['y'] + 0.5 * json_dict[target]['height']\n",
    "            grid_x = int(x * grid_data.shape[1])  # calculates grid location of target\n",
    "            grid_y = int(y * grid_data.shape[0])\n",
    "\n",
    "            cell_data = np.array([1, x, y, json_dict[target]['width'], json_dict[target]['height']])\n",
    "\n",
    "            grid_data[grid_y, grid_x] = cell_data\n",
    "        training_data.append([img_array, grid_data])\n",
    "    return np.array(training_data)\n",
    "\n",
    "\n",
    "def store_training_data():\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    box_training_data = create_training_data(img_path, data_path)\n",
    "    for feature, label in box_training_data:\n",
    "        X_data.append(feature)\n",
    "        y_data.append(label)\n",
    "    X = np.array(X_data)\n",
    "    print(X[:, :, :].shape)\n",
    "    X = X.reshape(-1, 6000, 6000, 3)\n",
    "    y = np.array(y_data)\n",
    "\n",
    "    pickle_out = open(r\"Training_Data/bounding_box_X.pickle\", \"wb\")\n",
    "    pickle.dump(X, pickle_out)\n",
    "    pickle_out.close()\n",
    "    pickle_out = open(r\"Training_Data/bounding_box_y.pickle\", \"wb\")\n",
    "    pickle.dump(y, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    mask_shape = tf.shape(y_true)[:3]\n",
    "\n",
    "    cell_x = tf.cast(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)), tf.float32)\n",
    "    cell_y = tf.transpose(cell_x, (0, 2, 1, 3, 4))\n",
    "\n",
    "    cell_grid = tf.tile(tf.concat([cell_x, cell_y], -1), [BATCH_SIZE, 1, 1, 5, 1])\n",
    "\n",
    "    conf_mask = tf.zeros(mask_shape)\n",
    "    # coord_mask = tf.zeros(mask_shape)\n",
    "\n",
    "    # seen = tf.Variable(0.)\n",
    "    # total_recall = tf.Variable(0.)\n",
    "\n",
    "    pred_box_xy = tf.sigmoid(y_pred[..., 1:3])  # + cell_grid\n",
    "    pred_box_wh = tf.exp(y_pred[..., 3:])  # * np.reshape(ANCHORS, [1, 1, 1, BOX, 2])\n",
    "\n",
    "    pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
    "\n",
    "    true_box_xy = y_true[..., 1:3]\n",
    "    true_box_wh = y_true[..., 3:]\n",
    "\n",
    "    true_wh_half = true_box_wh / 2.\n",
    "    true_mins = true_box_xy - true_wh_half\n",
    "    true_maxes = true_box_xy + true_wh_half\n",
    "\n",
    "    pred_wh_half = pred_box_wh / 2.\n",
    "    pred_mins = pred_box_xy - pred_wh_half\n",
    "    pred_maxes = pred_box_xy - pred_wh_half\n",
    "\n",
    "    intersect_mins = tf.maximum(pred_mins, true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "\n",
    "    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
    "    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores = tf.truediv(intersect_areas, union_areas)\n",
    "    best_ious = tf.reduce_max(iou_scores)\n",
    "\n",
    "    true_box_conf = iou_scores * y_true[..., 0]\n",
    "\n",
    "    coord_mask = tf.expand_dims(y_true[..., 0], axis=-1) * COORD_SCALE\n",
    "\n",
    "    conf_mask = conf_mask + tf.cast(best_ious < 0.6, tf.float32) * (1 - y_true[..., 0]) * NO_OBJECT_SCALE\n",
    "\n",
    "    conf_mask = conf_mask + y_true[..., 0] * OBJECT_SCALE\n",
    "\n",
    "    # warm up training\n",
    "    # no_boxes_mask = tf.cast(coord_mask < COORD_SCALE/2., tf.float32)\n",
    "    # seen.assign_add(1.)\n",
    "    #\n",
    "    # true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, WARM_UP_BATCHES),\n",
    "    #                         lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask])\n",
    "\n",
    "    nb_coord_box = tf.reduce_sum(tf.cast(coord_mask > 0.0, tf.float32))\n",
    "    nb_conf_box = tf.reduce_sum(tf.cast(conf_mask > 0.0, tf.float32))\n",
    "\n",
    "    loss_xy = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy) * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_wh = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh) * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_conf = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask) / (nb_conf_box + 1e-6) / 2.\n",
    "\n",
    "    loss = loss_xy + loss_wh + loss_conf\n",
    "\n",
    "    # print(true_box_xy)\n",
    "    # nb_true_box = tf.reduce_sum(y_true[..., 0])\n",
    "    # nb_pred_box = tf.reduce_sum(tf.cast(true_box_conf > 0.5, tf.float32)) * tf.cast(pred_box_conf > 0.3, tf.float32)\n",
    "\n",
    "    # current_recall = nb_pred_box / (nb_true_box + 1e-6)\n",
    "    # total_recall.assign_add(current_recall)\n",
    "\n",
    "    # loss = tf.print(loss, [tf.zeros(1)], message='Dummy line \\t', summarize=1000)\n",
    "    # loss = tf.print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
    "    # loss = tf.print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
    "    # loss = tf.print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
    "    # loss = tf.print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
    "    # loss = tf.print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
    "    # loss = tf.print(loss, [total_recall/seen], message='Average Recall \\t', summarize=1000)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    pickle_in = open(\"Training_Data/bounding_box_X.pickle\", 'rb')\n",
    "    X = pickle.load(pickle_in)\n",
    "\n",
    "    pickle_in = open(\"Training_Data/bounding_box_y.pickle\", 'rb')\n",
    "    y = pickle.load(pickle_in)\n",
    "\n",
    "    X = X/255.\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (7, 7), input_shape=X.shape[1:], strides=2))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "    model.add(Conv2D(192, (5, 5), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "    model.add(Conv2D(128, (5, 5), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "    model.add(Conv2D(128, (5, 5), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "    # model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "    # model.add(LeakyReLU(alpha=0.05))\n",
    "    # model.add(Conv2D(256, (1, 1), padding='same'))\n",
    "    # model.add(LeakyReLU(alpha=0.05))\n",
    "    # model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "    # model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "    # model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "    # model.add(LeakyReLU(alpha=0.05))\n",
    "    # model.add(Conv2D(256, (1, 1), padding='same'))\n",
    "    # model.add(LeakyReLU(alpha=0.05))\n",
    "    # model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "    # model.add(LeakyReLU(alpha=0.05))\n",
    "    # model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "    # model.add(LeakyReLU(alpha=0.05))\n",
    "    # model.add(Conv2D(512, (1, 1), padding='same'))\n",
    "    # model.add(LeakyReLU(alpha=0.05))\n",
    "    # model.add(Conv2D(1024, (3, 3), padding='same'))\n",
    "    # model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "    model.add(Conv2D(128, (1, 1), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "#     model.add(Conv2D(1024, (3, 3)))\n",
    "#     model.add(LeakyReLU(alpha=0.05))\n",
    "#     model.add(Conv2D(1024, (3, 3)))\n",
    "#     model.add(LeakyReLU(alpha=0.05))\n",
    "#     model.add(Conv2D(1024, (3, 3)))\n",
    "\n",
    "#     model.add(Conv2D(1024, (3, 3), padding='same'))\n",
    "#     model.add(LeakyReLU(alpha=0.05))\n",
    "#     model.add(Conv2D(1024, (3, 3), padding='same'))\n",
    "#     model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(Conv2D(128, (3, 3), strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(Conv2D(128, (3, 3), strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(Conv2D(64, (3, 3), strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # model.add(Dense(4096))\n",
    "    # model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(GRID_H * GRID_W * 5))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Reshape((GRID_H, GRID_W, 5)))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss=custom_loss,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X, y,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=4,\n",
    "              validation_split=0.2,\n",
    "              )\n",
    "    model.save('bounding_box.model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.43it/s]\n",
      "C:\\Users\\hscot\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 6000, 6000, 3)\n"
     ]
    }
   ],
   "source": [
    "store_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 2997, 2997, 64)    9472      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 2997, 2997, 64)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 1498, 1498, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 1498, 1498, 192)   307392    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 1498, 1498, 192)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 749, 749, 192)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 749, 749, 128)     614528    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 749, 749, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 374, 374, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 374, 374, 128)     409728    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 374, 374, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 187, 187, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 93, 93, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 46, 46, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 46, 46, 128)       16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 46, 46, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 23, 23, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 23, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 6, 6, 64)          73792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 180)               414900    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 6, 6, 5)           0         \n",
      "=================================================================\n",
      "Total params: 2,141,492\n",
      "Trainable params: 2,141,492\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/4\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.7)",
   "language": "python",
   "name": "3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
